{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype-Based 3D Brain Tumor Segmentation - Training\n",
    "\n",
    "Three-phase training for PrototypeSegNet3D:\n",
    "1. **Phase 1**: Warm-up (frozen backbone)\n",
    "2. **Phase 2**: Joint fine-tuning\n",
    "3. **Phase 3**: Prototype projection & refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATE THIS WITH YOUR GITHUB REPOSITORY URL\n",
    "# ============================================================================\n",
    "GITHUB_REPO_URL = \"https://github.com/dariamarc/brainTumorSurvival.git\"\n",
    "# ============================================================================\n",
    "\n",
    "repo_name = GITHUB_REPO_URL.split('/')[-1].replace('.git', '')\n",
    "\n",
    "os.chdir('/content')\n",
    "\n",
    "if os.path.exists(f'/content/{repo_name}'):\n",
    "    !rm -rf /content/{repo_name}\n",
    "\n",
    "!git clone {GITHUB_REPO_URL}\n",
    "\n",
    "os.chdir(f'/content/{repo_name}')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Download Data from AWS S3\n\nDownloads the preprocessed data as a single zip file for faster, more reliable transfer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import userdata\nimport os\n\n# Install AWS CLI\n!pip install -q awscli\n\n# ============================================================================\n# UPDATE THESE WITH YOUR S3 DETAILS\n# ============================================================================\nS3_BUCKET = 'your-brats2020-data'\nS3_ZIP_FILE = 'preprocessed_data_cropped.zip'\nAWS_REGION = 'eu-central-1'\n# ============================================================================\n\nLOCAL_PATH = '/content/brainTumorData_preprocessed_cropped'\nZIP_PATH = '/content/preprocessed_data_cropped.zip'\n\n# Load AWS credentials from Colab Secrets\nos.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\nos.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\nos.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n\n# Download zip file from S3 (single file transfer - much faster)\nprint(\"Downloading zip file from S3...\")\n!aws s3 cp s3://{S3_BUCKET}/{S3_ZIP_FILE} {ZIP_PATH}\n\n# Unzip the data\nprint(\"Extracting data...\")\n!mkdir -p {LOCAL_PATH}\n!unzip -q {ZIP_PATH} -d {LOCAL_PATH}\n\n# Clean up zip file to save disk space\n!rm {ZIP_PATH}\n\n# Check if files are in a nested folder (depends on how zip was created)\nh5_files = [f for f in os.listdir(LOCAL_PATH) if f.endswith('.h5')]\nif len(h5_files) == 0:\n    # Files might be in a subdirectory\n    subdirs = [d for d in os.listdir(LOCAL_PATH) if os.path.isdir(os.path.join(LOCAL_PATH, d))]\n    if subdirs:\n        nested_path = os.path.join(LOCAL_PATH, subdirs[0])\n        h5_files = [f for f in os.listdir(nested_path) if f.endswith('.h5')]\n        if h5_files:\n            print(f\"Found files in nested folder: {subdirs[0]}\")\n            LOCAL_PATH = nested_path\n\n# Verify file count\nfile_count = len([f for f in os.listdir(LOCAL_PATH) if f.endswith('.h5')])\nexpected_count = 369 * 128\nprint(f\"Found {file_count} files (expected {expected_count})\")\n\nDATA_PATH = LOCAL_PATH\nprint(f\"Data ready at: {DATA_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "repo_dir = f'/content/{repo_name}'\n",
    "resnet_dir = f'/content/{repo_name}/ResNet_architecture'\n",
    "\n",
    "sys.path.insert(0, repo_dir)\n",
    "sys.path.insert(0, resnet_dir)\n",
    "\n",
    "from prototype_segnet3d import create_prototype_segnet3d\n",
    "from trainer import PrototypeTrainer\n",
    "from data_processing.data_generator import MRIDataGenerator\n",
    "\n",
    "print(\"Modules imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# TRAINING CONFIGURATION\n# ============================================================================\n\n# Data\nBATCH_SIZE = 1\nSPLIT_RATIO = 0.2\nRANDOM_STATE = 42\nNUM_VOLUMES = 369\nNUM_SLICES = 128  # Preprocessed data has 128 slices per volume\n\n# Volume dimensions (must match preprocessed data)\n# Preprocessing crops from (155, 240, 240) to (128, 160, 192)\nD = 128  # Depth (number of slices)\nH = 160  # Height (cropped from 240)\nW = 192  # Width (cropped from 240)\nC = 4    # Channels (FLAIR, T1, T1ce, T2)\n\nNUM_CLASSES = 4\nN_PROTOTYPES = 3\n\n# Model\nBACKBONE_CHANNELS = 64\nASPP_OUT_CHANNELS = 256\nDILATION_RATES = (2, 4, 8)\n\n# Training epochs per phase\nPHASE1_EPOCHS = 50\nPHASE2_EPOCHS = 150\nPHASE3_EPOCHS = 30\n\n# ============================================================================\n\nINPUT_SHAPE = (D, H, W, C)\nprint(f\"Input shape: {INPUT_SHAPE}\")\nprint(f\"Classes: {NUM_CLASSES}, Prototypes: {N_PROTOTYPES}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_generator = MRIDataGenerator(\n    DATA_PATH,\n    batch_size=BATCH_SIZE,\n    num_slices=NUM_SLICES,\n    num_volumes=NUM_VOLUMES,\n    split_ratio=SPLIT_RATIO,\n    subset='train',\n    shuffle=True,\n    random_state=RANDOM_STATE\n)\n\nval_generator = MRIDataGenerator(\n    DATA_PATH,\n    batch_size=BATCH_SIZE,\n    num_slices=NUM_SLICES,\n    num_volumes=NUM_VOLUMES,\n    split_ratio=SPLIT_RATIO,\n    subset='val',\n    shuffle=False,\n    random_state=RANDOM_STATE\n)\n\nprint(f\"Training batches: {len(train_generator)}\")\nprint(f\"Validation batches: {len(val_generator)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_prototype_segnet3d(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    n_prototypes=N_PROTOTYPES,\n",
    "    backbone_channels=BACKBONE_CHANNELS,\n",
    "    aspp_out_channels=ASPP_OUT_CHANNELS,\n",
    "    dilation_rates=DILATION_RATES,\n",
    "    distance_type='l2',\n",
    "    activation_function='log'\n",
    ")\n",
    "\n",
    "# Initialize weights\n",
    "dummy_input = tf.zeros((1,) + INPUT_SHAPE)\n",
    "_ = model(dummy_input, training=False)\n",
    "\n",
    "print(\"Model built.\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Setup Google Drive for Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import datetime\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "DRIVE_CHECKPOINT_DIR = f'/content/drive/MyDrive/prototype_segnet_checkpoints/{timestamp}'\n",
    "LOCAL_CHECKPOINT_DIR = '/content/checkpoints'\n",
    "\n",
    "!mkdir -p {DRIVE_CHECKPOINT_DIR}\n",
    "!mkdir -p {LOCAL_CHECKPOINT_DIR}\n",
    "\n",
    "print(f\"Drive checkpoint dir: {DRIVE_CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PrototypeTrainer(\n",
    "    model=model,\n",
    "    train_generator=train_generator,\n",
    "    val_generator=val_generator,\n",
    "    checkpoint_dir=LOCAL_CHECKPOINT_DIR\n",
    ")\n",
    "\n",
    "print(\"Trainer created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Phase 1 - Warm-up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase1(epochs=PHASE1_EPOCHS)\n",
    "\n",
    "# Save to Google Drive\n",
    "model.save(f'{DRIVE_CHECKPOINT_DIR}/model_after_phase1.keras')\n",
    "print(f\"Phase 1 model saved to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Phase 2 - Joint Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase2(epochs=PHASE2_EPOCHS)\n",
    "\n",
    "# Save to Google Drive\n",
    "model.save(f'{DRIVE_CHECKPOINT_DIR}/model_after_phase2.keras')\n",
    "print(f\"Phase 2 model saved to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Phase 3 - Prototype Projection & Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase3(epochs=PHASE3_EPOCHS)\n",
    "\n",
    "# Save final model to Google Drive\n",
    "model.save(f'{DRIVE_CHECKPOINT_DIR}/model_final.keras')\n",
    "print(f\"Final model saved to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "history = trainer.get_full_history()\n",
    "\n",
    "with open(f'{DRIVE_CHECKPOINT_DIR}/training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"Training complete. All files saved to: {DRIVE_CHECKPOINT_DIR}\")\n",
    "!ls -la {DRIVE_CHECKPOINT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 13: Plot Training Metrics\n\nVisualize Dice scores, purity ratios, and loss curves across all training phases.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Get plottable history\nhistory = trainer.get_full_history()\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: Dice Scores\nax1 = axes[0, 0]\nax1.plot(history['epochs'], history['dice_gd_enhancing'], label='GD-Enhancing', alpha=0.8)\nax1.plot(history['epochs'], history['dice_edema'], label='Edema', alpha=0.8)\nax1.plot(history['epochs'], history['dice_necrotic'], label='Necrotic', alpha=0.8)\nax1.plot(history['epochs'], history['dice_mean'], label='Mean', linewidth=2, color='black')\nfor boundary in history['phase_boundaries']:\n    ax1.axvline(x=boundary, color='gray', linestyle='--', alpha=0.5)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Dice Score')\nax1.set_title('Dice Scores by Class')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Purity Ratios\nax2 = axes[0, 1]\nax2.plot(history['epochs'], history['purity_proto_0'], label='Proto 0 (GD-Enh)', alpha=0.8)\nax2.plot(history['epochs'], history['purity_proto_1'], label='Proto 1 (Edema)', alpha=0.8)\nax2.plot(history['epochs'], history['purity_proto_2'], label='Proto 2 (Necrotic)', alpha=0.8)\nax2.plot(history['epochs'], history['purity_mean'], label='Mean', linewidth=2, color='black')\nfor boundary in history['phase_boundaries']:\n    ax2.axvline(x=boundary, color='gray', linestyle='--', alpha=0.5)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Purity Ratio')\nax2.set_title('Prototype Purity Ratios')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Training and Validation Loss\nax3 = axes[1, 0]\nax3.plot(history['epochs'], history['train_loss'], label='Train Loss', alpha=0.8)\nax3.plot(history['epochs'], history['val_loss'], label='Val Loss', alpha=0.8)\nfor boundary in history['phase_boundaries']:\n    ax3.axvline(x=boundary, color='gray', linestyle='--', alpha=0.5)\nax3.set_xlabel('Epoch')\nax3.set_ylabel('Loss')\nax3.set_title('Training and Validation Loss')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Whole Tumor Dice\nax4 = axes[1, 1]\nax4.plot(history['epochs'], history['dice_whole_tumor'], label='Whole Tumor Dice', color='green', linewidth=2)\nfor boundary in history['phase_boundaries']:\n    ax4.axvline(x=boundary, color='gray', linestyle='--', alpha=0.5)\nax4.set_xlabel('Epoch')\nax4.set_ylabel('Dice Score')\nax4.set_title('Whole Tumor Dice Score')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\n# Add phase labels\nfor ax in axes.flat:\n    if history['phase_boundaries']:\n        # Phase 1\n        ax.text(history['phase_boundaries'][0] / 2, ax.get_ylim()[1] * 0.95, \n                'Phase 1', ha='center', fontsize=9, alpha=0.7)\n        # Phase 2\n        if len(history['phase_boundaries']) > 1:\n            mid_phase2 = (history['phase_boundaries'][0] + history['phase_boundaries'][1]) / 2\n            ax.text(mid_phase2, ax.get_ylim()[1] * 0.95, \n                    'Phase 2', ha='center', fontsize=9, alpha=0.7)\n            # Phase 3\n            mid_phase3 = (history['phase_boundaries'][1] + max(history['epochs'])) / 2\n            ax.text(mid_phase3, ax.get_ylim()[1] * 0.95, \n                    'Phase 3', ha='center', fontsize=9, alpha=0.7)\n\nplt.tight_layout()\nplt.savefig(f'{DRIVE_CHECKPOINT_DIR}/training_metrics.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"Plot saved to: {DRIVE_CHECKPOINT_DIR}/training_metrics.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}