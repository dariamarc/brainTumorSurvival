{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype-Based 3D Brain Tumor Segmentation - Training\n",
    "\n",
    "Three-phase training for PrototypeSegNet3D:\n",
    "1. **Phase 1**: Warm-up (frozen backbone)\n",
    "2. **Phase 2**: Joint fine-tuning\n",
    "3. **Phase 3**: Prototype projection & refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATE THIS WITH YOUR GITHUB REPOSITORY URL\n",
    "# ============================================================================\n",
    "GITHUB_REPO_URL = \"https://github.com/dariamarc/brainTumorSurvival.git\"\n",
    "# ============================================================================\n",
    "\n",
    "repo_name = GITHUB_REPO_URL.split('/')[-1].replace('.git', '')\n",
    "\n",
    "os.chdir('/content')\n",
    "\n",
    "if os.path.exists(f'/content/{repo_name}'):\n",
    "    !rm -rf /content/{repo_name}\n",
    "\n",
    "!git clone {GITHUB_REPO_URL}\n",
    "\n",
    "os.chdir(f'/content/{repo_name}')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Data from AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATE THESE WITH YOUR S3 DETAILS\n",
    "# ============================================================================\n",
    "S3_BUCKET = 'your-brats2020-data'\n",
    "S3_PATH = 'preprocessed_data'\n",
    "AWS_REGION = 'eu-central-1'\n",
    "# ============================================================================\n",
    "\n",
    "LOCAL_PATH = '/content/brainTumorData_preprocessed'\n",
    "\n",
    "# Load AWS credentials from Colab Secrets\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
    "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
    "\n",
    "!mkdir -p {LOCAL_PATH}\n",
    "!aws s3 sync s3://{S3_BUCKET}/{S3_PATH} {LOCAL_PATH}\n",
    "\n",
    "DATA_PATH = LOCAL_PATH\n",
    "print(f\"Data downloaded to: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "repo_dir = f'/content/{repo_name}'\n",
    "resnet_dir = f'/content/{repo_name}/ResNet_architecture'\n",
    "\n",
    "sys.path.insert(0, repo_dir)\n",
    "sys.path.insert(0, resnet_dir)\n",
    "\n",
    "from prototype_segnet3d import create_prototype_segnet3d\n",
    "from trainer import PrototypeTrainer\n",
    "from data_processing.data_generator import MRIDataGenerator\n",
    "\n",
    "print(\"Modules imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# TRAINING CONFIGURATION\n# ============================================================================\n\n# Data\nBATCH_SIZE = 1\nSPLIT_RATIO = 0.2\nRANDOM_STATE = 42\nNUM_VOLUMES = 369\nNUM_SLICES = 128  # Preprocessed data has 128 slices per volume\n\n# Volume dimensions (must match preprocessed data)\n# Preprocessing crops from (155, 240, 240) to (128, 160, 192)\nD = 128  # Depth (number of slices)\nH = 160  # Height (cropped from 240)\nW = 192  # Width (cropped from 240)\nC = 4    # Channels (FLAIR, T1, T1ce, T2)\n\nNUM_CLASSES = 4\nN_PROTOTYPES = 3\n\n# Model\nBACKBONE_CHANNELS = 64\nASPP_OUT_CHANNELS = 256\nDILATION_RATES = (2, 4, 8)\n\n# Training epochs per phase\nPHASE1_EPOCHS = 50\nPHASE2_EPOCHS = 150\nPHASE3_EPOCHS = 30\n\n# ============================================================================\n\nINPUT_SHAPE = (D, H, W, C)\nprint(f\"Input shape: {INPUT_SHAPE}\")\nprint(f\"Classes: {NUM_CLASSES}, Prototypes: {N_PROTOTYPES}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_generator = MRIDataGenerator(\n    DATA_PATH,\n    batch_size=BATCH_SIZE,\n    num_slices=NUM_SLICES,\n    num_volumes=NUM_VOLUMES,\n    split_ratio=SPLIT_RATIO,\n    subset='train',\n    shuffle=True,\n    random_state=RANDOM_STATE\n)\n\nval_generator = MRIDataGenerator(\n    DATA_PATH,\n    batch_size=BATCH_SIZE,\n    num_slices=NUM_SLICES,\n    num_volumes=NUM_VOLUMES,\n    split_ratio=SPLIT_RATIO,\n    subset='val',\n    shuffle=False,\n    random_state=RANDOM_STATE\n)\n\nprint(f\"Training batches: {len(train_generator)}\")\nprint(f\"Validation batches: {len(val_generator)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_prototype_segnet3d(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    n_prototypes=N_PROTOTYPES,\n",
    "    backbone_channels=BACKBONE_CHANNELS,\n",
    "    aspp_out_channels=ASPP_OUT_CHANNELS,\n",
    "    dilation_rates=DILATION_RATES,\n",
    "    distance_type='l2',\n",
    "    activation_function='log'\n",
    ")\n",
    "\n",
    "# Initialize weights\n",
    "dummy_input = tf.zeros((1,) + INPUT_SHAPE)\n",
    "_ = model(dummy_input, training=False)\n",
    "\n",
    "print(\"Model built.\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Setup Google Drive for Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import datetime\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "DRIVE_CHECKPOINT_DIR = f'/content/drive/MyDrive/prototype_segnet_checkpoints/{timestamp}'\n",
    "LOCAL_CHECKPOINT_DIR = '/content/checkpoints'\n",
    "\n",
    "!mkdir -p {DRIVE_CHECKPOINT_DIR}\n",
    "!mkdir -p {LOCAL_CHECKPOINT_DIR}\n",
    "\n",
    "print(f\"Drive checkpoint dir: {DRIVE_CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PrototypeTrainer(\n",
    "    model=model,\n",
    "    train_generator=train_generator,\n",
    "    val_generator=val_generator,\n",
    "    checkpoint_dir=LOCAL_CHECKPOINT_DIR\n",
    ")\n",
    "\n",
    "print(\"Trainer created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Phase 1 - Warm-up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase1(epochs=PHASE1_EPOCHS)\n",
    "\n",
    "# Save to Google Drive\n",
    "model.save(f'{DRIVE_CHECKPOINT_DIR}/model_after_phase1.keras')\n",
    "print(f\"Phase 1 model saved to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Phase 2 - Joint Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase2(epochs=PHASE2_EPOCHS)\n",
    "\n",
    "# Save to Google Drive\n",
    "model.save(f'{DRIVE_CHECKPOINT_DIR}/model_after_phase2.keras')\n",
    "print(f\"Phase 2 model saved to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Phase 3 - Prototype Projection & Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase3(epochs=PHASE3_EPOCHS)\n",
    "\n",
    "# Save final model to Google Drive\n",
    "model.save(f'{DRIVE_CHECKPOINT_DIR}/model_final.keras')\n",
    "print(f\"Final model saved to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "history = trainer.get_full_history()\n",
    "\n",
    "with open(f'{DRIVE_CHECKPOINT_DIR}/training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"Training complete. All files saved to: {DRIVE_CHECKPOINT_DIR}\")\n",
    "!ls -la {DRIVE_CHECKPOINT_DIR}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}