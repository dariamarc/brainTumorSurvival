{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype-Based 3D Brain Tumor Segmentation - SageMaker Training\n",
    "\n",
    "Three-phase training for PrototypeSegNet3D on AWS SageMaker.\n",
    "\n",
    "1. **Phase 1**: Warm-up (frozen backbone)\n",
    "2. **Phase 2**: Joint fine-tuning\n",
    "3. **Phase 3**: Prototype projection & refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install Dependencies"
  },
  {
   "cell_type": "code",
   "source": "# Install required packages\n!pip install -q tensorflow==2.15.0 h5py tqdm matplotlib numpy\n\n# Verify TensorFlow installation\nimport tensorflow as tf\nprint(f\"TensorFlow version: {tf.__version__}\")\n\n# Check GPU availability\ngpus = tf.config.list_physical_devices('GPU')\nprint(f\"GPUs available: {len(gpus)}\")\nfor gpu in gpus:\n    print(f\"  {gpu}\")\n\n# Enable memory growth to prevent TF from grabbing all GPU memory\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n    print(\"Memory growth enabled.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set working directory to the repo\n",
    "REPO_DIR = '/home/ec2-user/SageMaker/brainTumorSurvival'\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "# Add paths for imports\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "sys.path.insert(0, os.path.join(REPO_DIR, 'ResNet_architecture'))\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import zipfile\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATE THESE WITH YOUR S3 DETAILS\n",
    "# ============================================================================\n",
    "S3_BUCKET = 'your-brats2020-data'\n",
    "S3_ZIP_FILE = 'preprocessed_data_cropped.zip'\n",
    "# ============================================================================\n",
    "\n",
    "LOCAL_DATA_DIR = '/home/ec2-user/SageMaker/data'\n",
    "ZIP_PATH = os.path.join(LOCAL_DATA_DIR, 'preprocessed_data_cropped.zip')\n",
    "DATA_PATH = os.path.join(LOCAL_DATA_DIR, 'preprocessed_data_cropped')\n",
    "\n",
    "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Check if data already exists (persists across notebook restarts)\n",
    "if os.path.exists(DATA_PATH) and len(os.listdir(DATA_PATH)) > 0:\n",
    "    h5_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.h5')]\n",
    "    if len(h5_files) > 40000:\n",
    "        print(f\"Data already exists: {len(h5_files)} files found\")\n",
    "        print(\"Skipping download.\")\n",
    "    else:\n",
    "        print(f\"Incomplete data found ({len(h5_files)} files). Re-downloading...\")\n",
    "        need_download = True\n",
    "else:\n",
    "    need_download = True\n",
    "\n",
    "if 'need_download' in dir() and need_download:\n",
    "    print(f\"Downloading data from s3://{S3_BUCKET}/{S3_ZIP_FILE}...\")\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(S3_BUCKET, S3_ZIP_FILE, ZIP_PATH)\n",
    "    print(\"Download complete.\")\n",
    "    \n",
    "    # Extract\n",
    "    print(\"Extracting...\")\n",
    "    os.makedirs(DATA_PATH, exist_ok=True)\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_PATH)\n",
    "    \n",
    "    # Remove zip to save space\n",
    "    os.remove(ZIP_PATH)\n",
    "    print(\"Extraction complete.\")\n",
    "    \n",
    "    # Check for nested folder\n",
    "    h5_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.h5')]\n",
    "    if len(h5_files) == 0:\n",
    "        subdirs = [d for d in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, d))]\n",
    "        if subdirs:\n",
    "            DATA_PATH = os.path.join(DATA_PATH, subdirs[0])\n",
    "            h5_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.h5')]\n",
    "\n",
    "# Verify\n",
    "file_count = len([f for f in os.listdir(DATA_PATH) if f.endswith('.h5')])\n",
    "print(f\"Data ready: {file_count} files at {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPUs available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    print(f\"  {gpu}\")\n",
    "\n",
    "# Import project modules\n",
    "from prototype_segnet3d import create_prototype_segnet3d\n",
    "from trainer import PrototypeTrainer\n",
    "from data_processing.data_generator import MRIDataGenerator\n",
    "\n",
    "print(\"Modules imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Data\n",
    "BATCH_SIZE = 1\n",
    "SPLIT_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "NUM_VOLUMES = 369\n",
    "NUM_SLICES = 128\n",
    "\n",
    "# Volume dimensions\n",
    "D = 128\n",
    "H = 160\n",
    "W = 192\n",
    "C = 4\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "N_PROTOTYPES = 3\n",
    "\n",
    "# Model\n",
    "BACKBONE_CHANNELS = 64\n",
    "ASPP_OUT_CHANNELS = 256\n",
    "DILATION_RATES = (2, 4, 8)\n",
    "\n",
    "# Training epochs per phase\n",
    "PHASE1_EPOCHS = 50\n",
    "PHASE2_EPOCHS = 150\n",
    "PHASE3_EPOCHS = 30\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "INPUT_SHAPE = (D, H, W, C)\n",
    "print(f\"Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"Classes: {NUM_CLASSES}, Prototypes: {N_PROTOTYPES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Setup Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Local checkpoint directory (persists on the notebook volume)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "CHECKPOINT_DIR = f'/home/ec2-user/SageMaker/checkpoints/{timestamp}'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# S3 path for backup (optional but recommended)\n",
    "S3_CHECKPOINT_PREFIX = f'checkpoints/{timestamp}'\n",
    "\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"S3 backup path: s3://{S3_BUCKET}/{S3_CHECKPOINT_PREFIX}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = MRIDataGenerator(\n",
    "    DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_slices=NUM_SLICES,\n",
    "    num_volumes=NUM_VOLUMES,\n",
    "    split_ratio=SPLIT_RATIO,\n",
    "    subset='train',\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "val_generator = MRIDataGenerator(\n",
    "    DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_slices=NUM_SLICES,\n",
    "    num_volumes=NUM_VOLUMES,\n",
    "    split_ratio=SPLIT_RATIO,\n",
    "    subset='val',\n",
    "    shuffle=False,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_generator)}\")\n",
    "print(f\"Validation batches: {len(val_generator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_prototype_segnet3d(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    n_prototypes=N_PROTOTYPES,\n",
    "    backbone_channels=BACKBONE_CHANNELS,\n",
    "    aspp_out_channels=ASPP_OUT_CHANNELS,\n",
    "    dilation_rates=DILATION_RATES,\n",
    "    distance_type='l2',\n",
    "    activation_function='log'\n",
    ")\n",
    "\n",
    "# Initialize weights\n",
    "dummy_input = tf.zeros((1,) + INPUT_SHAPE)\n",
    "_ = model(dummy_input, training=False)\n",
    "\n",
    "print(\"Model built.\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PrototypeTrainer(\n",
    "    model=model,\n",
    "    train_generator=train_generator,\n",
    "    val_generator=val_generator,\n",
    "    checkpoint_dir=CHECKPOINT_DIR\n",
    ")\n",
    "\n",
    "print(\"Trainer created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Phase 1 - Warm-up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase1(epochs=PHASE1_EPOCHS)\n",
    "\n",
    "# Save model\n",
    "model.save(f'{CHECKPOINT_DIR}/model_after_phase1.keras')\n",
    "print(\"Phase 1 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Backup to S3 (After Phase 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup checkpoints to S3\n",
    "!aws s3 sync {CHECKPOINT_DIR} s3://{S3_BUCKET}/{S3_CHECKPOINT_PREFIX}/ --quiet\n",
    "print(f\"Phase 1 backed up to S3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Phase 2 - Joint Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase2(epochs=PHASE2_EPOCHS)\n",
    "\n",
    "# Save model\n",
    "model.save(f'{CHECKPOINT_DIR}/model_after_phase2.keras')\n",
    "print(\"Phase 2 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Backup to S3 (After Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync {CHECKPOINT_DIR} s3://{S3_BUCKET}/{S3_CHECKPOINT_PREFIX}/ --quiet\n",
    "print(f\"Phase 2 backed up to S3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Phase 3 - Prototype Projection & Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase3(epochs=PHASE3_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "model.save(f'{CHECKPOINT_DIR}/model_final.keras')\n",
    "print(\"Phase 3 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "history = trainer.get_full_history()\n",
    "\n",
    "with open(f'{CHECKPOINT_DIR}/training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"Training history saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Final Backup to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync {CHECKPOINT_DIR} s3://{S3_BUCKET}/{S3_CHECKPOINT_PREFIX}/ --quiet\n",
    "print(f\"All files backed up to: s3://{S3_BUCKET}/{S3_CHECKPOINT_PREFIX}/\")\n",
    "\n",
    "# List files\n",
    "!ls -la {CHECKPOINT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Plot Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = trainer.get_full_history()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Dice Scores\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['epochs'], history['dice_gd_enhancing'], label='GD-Enhancing', alpha=0.8)\n",
    "ax1.plot(history['epochs'], history['dice_edema'], label='Edema', alpha=0.8)\n",
    "ax1.plot(history['epochs'], history['dice_necrotic'], label='Necrotic', alpha=0.8)\n",
    "ax1.plot(history['epochs'], history['dice_mean'], label='Mean', linewidth=2, color='black')\n",
    "for boundary in history['phase_boundaries']:\n",
    "    ax1.axvline(x=boundary, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Dice Score')\n",
    "ax1.set_title('Dice Scores by Class')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Purity Ratios\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['epochs'], history['purity_proto_0'], label='Proto 0 (GD-Enh)', alpha=0.8)\n",
    "ax2.plot(history['epochs'], history['purity_proto_1'], label='Proto 1 (Edema)', alpha=0.8)\n",
    "ax2.plot(history['epochs'], history['purity_proto_2'], label='Proto 2 (Necrotic)', alpha=0.8)\n",
    "ax2.plot(history['epochs'], history['purity_mean'], label='Mean', linewidth=2, color='black')\n",
    "for boundary in history['phase_boundaries']:\n",
    "    ax2.axvline(x=boundary, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Purity Ratio')\n",
    "ax2.set_title('Prototype Purity Ratios')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Loss\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history['epochs'], history['train_loss'], label='Train Loss', alpha=0.8)\n",
    "ax3.plot(history['epochs'], history['val_loss'], label='Val Loss', alpha=0.8)\n",
    "for boundary in history['phase_boundaries']:\n",
    "    ax3.axvline(x=boundary, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Training and Validation Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Whole Tumor Dice\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history['epochs'], history['dice_whole_tumor'], label='Whole Tumor', color='green', linewidth=2)\n",
    "for boundary in history['phase_boundaries']:\n",
    "    ax4.axvline(x=boundary, color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Dice Score')\n",
    "ax4.set_title('Whole Tumor Dice Score')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHECKPOINT_DIR}/training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Visual Comparison - Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "colors = ['black', 'red', 'limegreen', 'dodgerblue']\n",
    "seg_cmap = ListedColormap(colors)\n",
    "\n",
    "def visualize_prediction(model, val_generator, sample_idx=0, slices_to_show=5):\n",
    "    images, masks = val_generator[sample_idx]\n",
    "    \n",
    "    logits, similarities = model(images, training=False)\n",
    "    predictions = tf.nn.softmax(logits, axis=-1)\n",
    "    pred_classes = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "    gt_classes = tf.argmax(masks, axis=-1).numpy()[0]\n",
    "    input_image = images[0, :, :, :, 0]\n",
    "    \n",
    "    depth = input_image.shape[0]\n",
    "    slice_indices = np.linspace(depth // 4, 3 * depth // 4, slices_to_show, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(slices_to_show, 4, figsize=(16, 4 * slices_to_show))\n",
    "    \n",
    "    for row, slice_idx in enumerate(slice_indices):\n",
    "        axes[row, 0].imshow(input_image[slice_idx], cmap='gray')\n",
    "        axes[row, 0].set_title(f'FLAIR (Slice {slice_idx})')\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        axes[row, 1].imshow(gt_classes[slice_idx], cmap=seg_cmap, vmin=0, vmax=3)\n",
    "        axes[row, 1].set_title('Ground Truth')\n",
    "        axes[row, 1].axis('off')\n",
    "        \n",
    "        axes[row, 2].imshow(pred_classes[slice_idx], cmap=seg_cmap, vmin=0, vmax=3)\n",
    "        axes[row, 2].set_title('Prediction')\n",
    "        axes[row, 2].axis('off')\n",
    "        \n",
    "        axes[row, 3].imshow(input_image[slice_idx], cmap='gray')\n",
    "        pred_overlay = np.zeros((*pred_classes[slice_idx].shape, 4))\n",
    "        for class_idx, color in enumerate([(0,0,0,0), (1,0,0,0.4), (0,1,0,0.4), (0,0,1,0.4)]):\n",
    "            mask = pred_classes[slice_idx] == class_idx\n",
    "            pred_overlay[mask] = color\n",
    "        axes[row, 3].imshow(pred_overlay)\n",
    "        axes[row, 3].set_title('Overlay')\n",
    "        axes[row, 3].axis('off')\n",
    "    \n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='red', markersize=10, label='GD-Enhancing'),\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='limegreen', markersize=10, label='Edema'),\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='dodgerblue', markersize=10, label='Necrotic')\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 1.02))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/prediction_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    from metrics import SegmentationMetrics\n",
    "    seg_metrics = SegmentationMetrics()\n",
    "    dice_scores = seg_metrics.compute_all(masks, logits)\n",
    "    \n",
    "    print(f\"\\nDice scores for this sample:\")\n",
    "    print(f\"  GD-Enhancing: {dice_scores['dice_gd_enhancing']:.3f}\")\n",
    "    print(f\"  Edema:        {dice_scores['dice_edema']:.3f}\")\n",
    "    print(f\"  Necrotic:     {dice_scores['dice_necrotic']:.3f}\")\n",
    "    print(f\"  Mean:         {dice_scores['dice_mean']:.3f}\")\n",
    "\n",
    "visualize_prediction(model, val_generator, sample_idx=0, slices_to_show=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Final S3 Sync (Including Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync {CHECKPOINT_DIR} s3://{S3_BUCKET}/{S3_CHECKPOINT_PREFIX}/\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"All outputs saved to: s3://{S3_BUCKET}/{S3_CHECKPOINT_PREFIX}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume Training (Optional)\n",
    "\n",
    "If the notebook disconnects, run these cells to resume from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and run this cell to resume from Phase 2 checkpoint\n",
    "# \n",
    "# RESUME_CHECKPOINT_DIR = '/home/ec2-user/SageMaker/checkpoints/YYYYMMDD-HHMMSS'  # Update this\n",
    "# \n",
    "# # Rebuild model\n",
    "# model = create_prototype_segnet3d(\n",
    "#     input_shape=INPUT_SHAPE,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     n_prototypes=N_PROTOTYPES,\n",
    "#     backbone_channels=BACKBONE_CHANNELS,\n",
    "#     aspp_out_channels=ASPP_OUT_CHANNELS,\n",
    "#     dilation_rates=DILATION_RATES,\n",
    "#     distance_type='l2',\n",
    "#     activation_function='log'\n",
    "# )\n",
    "# _ = model(tf.zeros((1,) + INPUT_SHAPE), training=False)\n",
    "# \n",
    "# # Load weights from checkpoint\n",
    "# checkpoint_path = f'{RESUME_CHECKPOINT_DIR}/model_after_phase2.keras'\n",
    "# loaded_model = tf.keras.models.load_model(checkpoint_path)\n",
    "# model.set_weights(loaded_model.get_weights())\n",
    "# print(f\"Weights loaded from {checkpoint_path}\")\n",
    "# \n",
    "# # Create new trainer and run Phase 3\n",
    "# trainer = PrototypeTrainer(\n",
    "#     model=model,\n",
    "#     train_generator=train_generator,\n",
    "#     val_generator=val_generator,\n",
    "#     checkpoint_dir=RESUME_CHECKPOINT_DIR\n",
    "# )\n",
    "# trainer.train_phase3(epochs=PHASE3_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.12 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}