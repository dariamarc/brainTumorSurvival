{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype-Based 3D Brain Tumor Segmentation - Paperspace Gradient\n",
    "\n",
    "Three-phase training for PrototypeSegNet3D on Paperspace Gradient.\n",
    "\n",
    "1. **Phase 1**: Warm-up (frozen backbone)\n",
    "2. **Phase 2**: Joint fine-tuning\n",
    "3. **Phase 3**: Prototype projection & refinement\n",
    "\n",
    "**Note**: Use `/storage` for persistent data that survives notebook restarts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Gradient notebooks start in /notebooks\n",
    "os.chdir('/notebooks')\n",
    "\n",
    "# Clone repo if not exists\n",
    "REPO_DIR = '/notebooks/brainTumorSurvival'\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/dariamarc/brainTumorSurvival.git\n",
    "else:\n",
    "    # Pull latest changes\n",
    "    os.chdir(REPO_DIR)\n",
    "    !git pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "REPO_DIR = '/notebooks/brainTumorSurvival'\n",
    "RESNET_DIR = f'{REPO_DIR}/ResNet_architecture'\n",
    "\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "sys.path.insert(0, RESNET_DIR)\n",
    "\n",
    "# Persistent storage for data and checkpoints (survives restarts)\n",
    "STORAGE_DIR = '/storage'\n",
    "DATA_DIR = f'{STORAGE_DIR}/brainTumorData_preprocessed_cropped'\n",
    "CHECKPOINT_BASE = f'{STORAGE_DIR}/checkpoints'\n",
    "\n",
    "os.makedirs(STORAGE_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_BASE, exist_ok=True)\n",
    "\n",
    "print(f\"Repo: {REPO_DIR}\")\n",
    "print(f\"Data will be stored at: {DATA_DIR}\")\n",
    "print(f\"Checkpoints will be stored at: {CHECKPOINT_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Data from S3\n",
    "\n",
    "Data is stored in `/storage` so it persists across notebook sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install boto3 for S3 access\n!pip install -q boto3\n\n# ============================================================================\n# UPDATE THESE WITH YOUR AWS CREDENTIALS AND S3 DETAILS\n# ============================================================================\nAWS_ACCESS_KEY_ID = 'YOUR_ACCESS_KEY'          # Replace with your key\nAWS_SECRET_ACCESS_KEY = 'YOUR_SECRET_KEY'      # Replace with your secret\nAWS_REGION = 'eu-central-1'\nS3_BUCKET = 'your-brats2020-data'              # Replace with your bucket\nS3_ZIP_FILE = 'preprocessed_data_cropped.zip'\n# ============================================================================"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport boto3\nfrom botocore.config import Config\n\nZIP_PATH = f'{STORAGE_DIR}/preprocessed_data_cropped.zip'\n\n# Check if data already exists\ndef count_h5_files(path):\n    if not os.path.exists(path):\n        return 0\n    return len([f for f in os.listdir(path) if f.endswith('.h5')])\n\nexisting_files = count_h5_files(DATA_DIR)\n\nif existing_files > 40000:\n    print(f\"Data already exists: {existing_files} files\")\n    print(\"Skipping download.\")\n    DATA_PATH = DATA_DIR\nelse:\n    print(f\"Downloading data from S3...\")\n    print(f\"  Bucket: {S3_BUCKET}\")\n    print(f\"  File: {S3_ZIP_FILE}\")\n    \n    # Create S3 client with credentials\n    s3_client = boto3.client(\n        's3',\n        aws_access_key_id=AWS_ACCESS_KEY_ID,\n        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n        region_name=AWS_REGION,\n        config=Config(signature_version='s3v4')\n    )\n    \n    # Download with progress\n    import sys\n    def download_progress(bytes_transferred):\n        sys.stdout.write(f\"\\r  Downloaded: {bytes_transferred / (1024**3):.2f} GB\")\n        sys.stdout.flush()\n    \n    class ProgressCallback:\n        def __init__(self):\n            self.bytes_transferred = 0\n        def __call__(self, bytes_amount):\n            self.bytes_transferred += bytes_amount\n            download_progress(self.bytes_transferred)\n    \n    progress = ProgressCallback()\n    s3_client.download_file(\n        S3_BUCKET, \n        S3_ZIP_FILE, \n        ZIP_PATH,\n        Callback=progress\n    )\n    print(\"\\n  Download complete!\")\n    \n    # Extract\n    print(\"Extracting...\")\n    os.makedirs(DATA_DIR, exist_ok=True)\n    \n    import zipfile\n    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n        zip_ref.extractall(DATA_DIR)\n    print(\"  Extraction complete!\")\n    \n    # Remove zip to save space\n    os.remove(ZIP_PATH)\n    print(\"  Cleaned up zip file.\")\n    \n    # Handle nested folder if present\n    h5_files = count_h5_files(DATA_DIR)\n    if h5_files == 0:\n        subdirs = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n        if subdirs:\n            nested = os.path.join(DATA_DIR, subdirs[0])\n            print(f\"  Moving files from nested folder: {subdirs[0]}\")\n            for f in os.listdir(nested):\n                os.rename(os.path.join(nested, f), os.path.join(DATA_DIR, f))\n            os.rmdir(nested)\n    \n    DATA_PATH = DATA_DIR\n\n# Verify\nfile_count = count_h5_files(DATA_PATH)\nprint(f\"\\nData ready: {file_count} files at {DATA_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check GPU and Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPUs available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    print(f\"  {gpu}\")\n",
    "\n",
    "# Show GPU memory\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prototype_segnet3d import create_prototype_segnet3d\n",
    "from trainer import PrototypeTrainer\n",
    "from data_processing.data_generator import MRIDataGenerator\n",
    "\n",
    "print(\"Modules imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Data\n",
    "BATCH_SIZE = 1\n",
    "SPLIT_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "NUM_VOLUMES = 369\n",
    "NUM_SLICES = 128\n",
    "\n",
    "# Volume dimensions\n",
    "D = 128\n",
    "H = 160\n",
    "W = 192\n",
    "C = 4\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "N_PROTOTYPES = 3\n",
    "\n",
    "# Model\n",
    "BACKBONE_CHANNELS = 64\n",
    "ASPP_OUT_CHANNELS = 256\n",
    "DILATION_RATES = (2, 4, 8)\n",
    "\n",
    "# Training epochs per phase\n",
    "PHASE1_EPOCHS = 50\n",
    "PHASE2_EPOCHS = 150\n",
    "PHASE3_EPOCHS = 30\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "INPUT_SHAPE = (D, H, W, C)\n",
    "print(f\"Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"Classes: {NUM_CLASSES}, Prototypes: {N_PROTOTYPES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setup Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "CHECKPOINT_DIR = f'{CHECKPOINT_BASE}/{timestamp}'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(\"(This is in /storage, so it persists across sessions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = MRIDataGenerator(\n",
    "    DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_slices=NUM_SLICES,\n",
    "    num_volumes=NUM_VOLUMES,\n",
    "    split_ratio=SPLIT_RATIO,\n",
    "    subset='train',\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "val_generator = MRIDataGenerator(\n",
    "    DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_slices=NUM_SLICES,\n",
    "    num_volumes=NUM_VOLUMES,\n",
    "    split_ratio=SPLIT_RATIO,\n",
    "    subset='val',\n",
    "    shuffle=False,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_generator)}\")\n",
    "print(f\"Validation batches: {len(val_generator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_prototype_segnet3d(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    n_prototypes=N_PROTOTYPES,\n",
    "    backbone_channels=BACKBONE_CHANNELS,\n",
    "    aspp_out_channels=ASPP_OUT_CHANNELS,\n",
    "    dilation_rates=DILATION_RATES,\n",
    "    distance_type='l2',\n",
    "    activation_function='log'\n",
    ")\n",
    "\n",
    "# Initialize weights\n",
    "dummy_input = tf.zeros((1,) + INPUT_SHAPE)\n",
    "_ = model(dummy_input, training=False)\n",
    "\n",
    "print(\"Model built.\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PrototypeTrainer(\n",
    "    model=model,\n",
    "    train_generator=train_generator,\n",
    "    val_generator=val_generator,\n",
    "    checkpoint_dir=CHECKPOINT_DIR\n",
    ")\n",
    "\n",
    "print(\"Trainer created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Phase 1 - Warm-up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase1(epochs=PHASE1_EPOCHS)\n",
    "\n",
    "model.save(f'{CHECKPOINT_DIR}/model_after_phase1.keras')\n",
    "print(\"Phase 1 complete and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Phase 2 - Joint Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase2(epochs=PHASE2_EPOCHS)\n",
    "\n",
    "model.save(f'{CHECKPOINT_DIR}/model_after_phase2.keras')\n",
    "print(\"Phase 2 complete and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Phase 3 - Prototype Projection & Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_phase3(epochs=PHASE3_EPOCHS)\n",
    "\n",
    "model.save(f'{CHECKPOINT_DIR}/model_final.keras')\n",
    "print(\"Phase 3 complete and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "history = trainer.get_full_history()\n",
    "\n",
    "with open(f'{CHECKPOINT_DIR}/training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(\"Training history saved.\")\n",
    "print(f\"\\nAll files saved to: {CHECKPOINT_DIR}\")\n",
    "!ls -la {CHECKPOINT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Plot Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = trainer.get_full_history()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Dice Scores\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['epochs'], history['dice_gd_enhancing'], label='GD-Enhancing', alpha=0.8)\n",
    "ax1.plot(history['epochs'], history['dice_edema'], label='Edema', alpha=0.8)\n",
    "ax1.plot(history['epochs'], history['dice_necrotic'], label='Necrotic', alpha=0.8)\n",
    "ax1.plot(history['epochs'], history['dice_mean'], label='Mean', linewidth=2, color='black')\n",
    "for b in history['phase_boundaries']:\n",
    "    ax1.axvline(x=b, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Dice Score')\n",
    "ax1.set_title('Dice Scores by Class')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Purity Ratios\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['epochs'], history['purity_proto_0'], label='Proto 0', alpha=0.8)\n",
    "ax2.plot(history['epochs'], history['purity_proto_1'], label='Proto 1', alpha=0.8)\n",
    "ax2.plot(history['epochs'], history['purity_proto_2'], label='Proto 2', alpha=0.8)\n",
    "ax2.plot(history['epochs'], history['purity_mean'], label='Mean', linewidth=2, color='black')\n",
    "for b in history['phase_boundaries']:\n",
    "    ax2.axvline(x=b, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Purity Ratio')\n",
    "ax2.set_title('Prototype Purity')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history['epochs'], history['train_loss'], label='Train', alpha=0.8)\n",
    "ax3.plot(history['epochs'], history['val_loss'], label='Val', alpha=0.8)\n",
    "for b in history['phase_boundaries']:\n",
    "    ax3.axvline(x=b, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Training and Validation Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Whole Tumor Dice\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history['epochs'], history['dice_whole_tumor'], color='green', linewidth=2)\n",
    "for b in history['phase_boundaries']:\n",
    "    ax4.axvline(x=b, color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Dice Score')\n",
    "ax4.set_title('Whole Tumor Dice')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHECKPOINT_DIR}/training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Visual Comparison - Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "colors = ['black', 'red', 'limegreen', 'dodgerblue']\n",
    "seg_cmap = ListedColormap(colors)\n",
    "\n",
    "def visualize_prediction(model, val_generator, sample_idx=0, slices_to_show=5):\n",
    "    images, masks = val_generator[sample_idx]\n",
    "    \n",
    "    logits, similarities = model(images, training=False)\n",
    "    predictions = tf.nn.softmax(logits, axis=-1)\n",
    "    pred_classes = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "    gt_classes = tf.argmax(masks, axis=-1).numpy()[0]\n",
    "    input_image = images[0, :, :, :, 0]\n",
    "    \n",
    "    depth = input_image.shape[0]\n",
    "    slice_indices = np.linspace(depth // 4, 3 * depth // 4, slices_to_show, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(slices_to_show, 4, figsize=(16, 4 * slices_to_show))\n",
    "    \n",
    "    for row, slice_idx in enumerate(slice_indices):\n",
    "        axes[row, 0].imshow(input_image[slice_idx], cmap='gray')\n",
    "        axes[row, 0].set_title(f'FLAIR (Slice {slice_idx})')\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        axes[row, 1].imshow(gt_classes[slice_idx], cmap=seg_cmap, vmin=0, vmax=3)\n",
    "        axes[row, 1].set_title('Ground Truth')\n",
    "        axes[row, 1].axis('off')\n",
    "        \n",
    "        axes[row, 2].imshow(pred_classes[slice_idx], cmap=seg_cmap, vmin=0, vmax=3)\n",
    "        axes[row, 2].set_title('Prediction')\n",
    "        axes[row, 2].axis('off')\n",
    "        \n",
    "        axes[row, 3].imshow(input_image[slice_idx], cmap='gray')\n",
    "        pred_overlay = np.zeros((*pred_classes[slice_idx].shape, 4))\n",
    "        for class_idx, color in enumerate([(0,0,0,0), (1,0,0,0.4), (0,1,0,0.4), (0,0,1,0.4)]):\n",
    "            mask = pred_classes[slice_idx] == class_idx\n",
    "            pred_overlay[mask] = color\n",
    "        axes[row, 3].imshow(pred_overlay)\n",
    "        axes[row, 3].set_title('Overlay')\n",
    "        axes[row, 3].axis('off')\n",
    "    \n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='red', markersize=10, label='GD-Enhancing'),\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='limegreen', markersize=10, label='Edema'),\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='dodgerblue', markersize=10, label='Necrotic')\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 1.02))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/prediction_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    from metrics import SegmentationMetrics\n",
    "    seg_metrics = SegmentationMetrics()\n",
    "    dice_scores = seg_metrics.compute_all(masks, logits)\n",
    "    \n",
    "    print(f\"\\nDice scores for this sample:\")\n",
    "    print(f\"  GD-Enhancing: {dice_scores['dice_gd_enhancing']:.3f}\")\n",
    "    print(f\"  Edema:        {dice_scores['dice_edema']:.3f}\")\n",
    "    print(f\"  Necrotic:     {dice_scores['dice_necrotic']:.3f}\")\n",
    "    print(f\"  Mean:         {dice_scores['dice_mean']:.3f}\")\n",
    "\n",
    "visualize_prediction(model, val_generator, sample_idx=0, slices_to_show=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Backup to S3 (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Backup checkpoints to S3\nimport boto3\nfrom botocore.config import Config\n\ns3_client = boto3.client(\n    's3',\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n    region_name=AWS_REGION,\n    config=Config(signature_version='s3v4')\n)\n\nS3_BACKUP_PREFIX = f'gradient_checkpoints/{timestamp}'\n\nprint(f\"Backing up to s3://{S3_BUCKET}/{S3_BACKUP_PREFIX}/\")\n\n# Upload all files in checkpoint directory\nfor filename in os.listdir(CHECKPOINT_DIR):\n    filepath = os.path.join(CHECKPOINT_DIR, filename)\n    if os.path.isfile(filepath):\n        s3_key = f\"{S3_BACKUP_PREFIX}/{filename}\"\n        print(f\"  Uploading: {filename}\")\n        s3_client.upload_file(filepath, S3_BUCKET, s3_key)\n\nprint(f\"\\nBackup complete: s3://{S3_BUCKET}/{S3_BACKUP_PREFIX}/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume Training (If Session Restarts)\n",
    "\n",
    "Since data and checkpoints are in `/storage`, they persist across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to resume from a checkpoint\n",
    "# \n",
    "# RESUME_CHECKPOINT_DIR = '/storage/checkpoints/YYYYMMDD-HHMMSS'  # Update this\n",
    "# \n",
    "# # Rebuild model\n",
    "# model = create_prototype_segnet3d(\n",
    "#     input_shape=INPUT_SHAPE,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     n_prototypes=N_PROTOTYPES,\n",
    "#     backbone_channels=BACKBONE_CHANNELS,\n",
    "#     aspp_out_channels=ASPP_OUT_CHANNELS,\n",
    "#     dilation_rates=DILATION_RATES,\n",
    "#     distance_type='l2',\n",
    "#     activation_function='log'\n",
    "# )\n",
    "# _ = model(tf.zeros((1,) + INPUT_SHAPE), training=False)\n",
    "# \n",
    "# # Load weights\n",
    "# checkpoint_file = f'{RESUME_CHECKPOINT_DIR}/model_after_phase2.keras'\n",
    "# loaded = tf.keras.models.load_model(checkpoint_file)\n",
    "# model.set_weights(loaded.get_weights())\n",
    "# print(f\"Loaded weights from: {checkpoint_file}\")\n",
    "# \n",
    "# # Create trainer and continue\n",
    "# trainer = PrototypeTrainer(\n",
    "#     model=model,\n",
    "#     train_generator=train_generator,\n",
    "#     val_generator=val_generator,\n",
    "#     checkpoint_dir=RESUME_CHECKPOINT_DIR\n",
    "# )\n",
    "# trainer.train_phase3(epochs=PHASE3_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Checkpoints Locally\n",
    "\n",
    "To download your trained model from Gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip of the checkpoint directory for easy download\n",
    "import shutil\n",
    "\n",
    "zip_name = f'/notebooks/checkpoint_{timestamp}'\n",
    "shutil.make_archive(zip_name, 'zip', CHECKPOINT_DIR)\n",
    "print(f\"Created: {zip_name}.zip\")\n",
    "print(\"You can download this file from the Gradient file browser.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}