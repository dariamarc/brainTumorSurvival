{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG4F1HK4DpjO"
   },
   "source": [
    "# 3D Brain Tumor Segmentation with MProtoNet - AWS S3 Deployment\n",
    "\n",
    "This notebook deploys your 3D brain tumor segmentation network from GitHub with data from AWS S3.\n",
    "\n",
    "**Quick Setup:**\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Go to Runtime ‚Üí Change runtime type ‚Üí Select **GPU (T4 or better)**\n",
    "3. Add AWS credentials to Colab Secrets (üîë icon on left)\n",
    "4. Update GitHub repository URL and S3 bucket details\n",
    "5. Run all cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsCiTQhCDpjT"
   },
   "source": [
    "## Step 1: Check GPU and System Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTsy0HQfDpjT",
    "outputId": "59041c7e-6189-405f-c208-1ccc45cc082a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM INFORMATION\n",
      "============================================================\n",
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "TensorFlow version: 2.19.0\n",
      "\n",
      "GPU Devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "‚úì Memory growth enabled for 1 GPU(s)\n",
      "‚úì GPU: /physical_device:GPU:0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"\\nGPU Devices: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Enable memory growth to prevent OOM errors\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"\\n‚úì Memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "        print(f\"‚úì GPU: {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error enabling memory growth: {e}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: No GPU detected! Please enable GPU in Runtime ‚Üí Change runtime type\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bDAKwVcDpjV"
   },
   "source": [
    "## Step 2: Check Available Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ii3s9VYNDpjV",
    "outputId": "391d44c5-d21a-4bc6-b370-b3ba48d65e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AVAILABLE RESOURCES\n",
      "============================================================\n",
      "\n",
      "üì¶ Disk Space:\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "\n",
      "üß† RAM:\n",
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:            52Gi       1.4Gi        47Gi       1.0Mi       3.7Gi        50Gi\n",
      "\n",
      "üéÆ GPU Memory:\n",
      "name, memory.total [MiB], memory.free [MiB]\n",
      "NVIDIA L4, 23034 MiB, 22689 MiB\n",
      "\n",
      "============================================================\n",
      "Note: Your 7.95 GB dataset will use disk space, not RAM\n",
      "Only one batch (~200 MB) is loaded in RAM at a time\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"AVAILABLE RESOURCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüì¶ Disk Space:\")\n",
    "!df -h /content | grep -E 'Filesystem|/content'\n",
    "\n",
    "print(\"\\nüß† RAM:\")\n",
    "!free -h | grep -E 'total|Mem'\n",
    "\n",
    "print(\"\\nüéÆ GPU Memory:\")\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Note: Your 7.95 GB dataset will use disk space, not RAM\")\n",
    "print(\"Only one batch (~200 MB) is loaded in RAM at a time\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUS4ypgnDpjV"
   },
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lY7bElMJDpjV"
   },
   "outputs": [],
   "source": "%%capture\n# Silent installation - remove %%capture to see output\n!pip install h5py numpy tensorflow keras matplotlib awscli boto3 scipy tqdm -q"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i_1J9XvDpjW"
   },
   "source": [
    "## Step 4: Clone GitHub Repository\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Update the repository URL below with your GitHub repository!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQYyQ5VBDpjW",
    "outputId": "45c72505-aa20-4eb7-ed78-024af1c104c1"
   },
   "outputs": [],
   "source": "import os\n\n# ============================================================================\n# UPDATE THIS WITH YOUR GITHUB REPOSITORY URL\n# ============================================================================\nGITHUB_REPO_URL = \"https://github.com/dariamarc/brainTumorSurvival.git\"\n# ============================================================================\n\n# Repository name (extracted from URL)\nrepo_name = GITHUB_REPO_URL.split('/')[-1].replace('.git', '')\n\nprint(f\"Cloning repository: {GITHUB_REPO_URL}\")\nprint(f\"Repository name: {repo_name}\")\nprint(\"-\" * 60)\n\n# IMPORTANT: Change to /content first to avoid directory issues\nos.chdir('/content')\nprint(\"Changed to /content directory\")\n\n# Remove if exists (for re-running)\nif os.path.exists(f'/content/{repo_name}'):\n    !rm -rf /content/{repo_name}\n    print(f\"Removed existing directory: {repo_name}\")\n\n# Clone the repository\n!git clone {GITHUB_REPO_URL}\n\n# Change to repository directory\nos.chdir(f'/content/{repo_name}')\nprint(f\"\\n‚úì Changed to directory: {os.getcwd()}\")\n\n# List files to verify\nprint(\"\\nRepository contents:\")\n!ls -la"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSjGON68DpjW"
   },
   "source": [
    "## Step 5: Verify Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2jBki6PDpjW",
    "outputId": "9201a736-91ce-46c5-fcd8-ab7d3f4bf1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking required files...\n",
      "============================================================\n",
      "‚úì model.py - Found\n",
      "‚úì data_generator.py - Found\n",
      "‚úì losses.py - Found\n",
      "\n",
      "Checking optional files...\n",
      "‚úì main.py - Found\n",
      "- train.py - Not present (optional)\n",
      "============================================================\n",
      "‚úì All required files present! Ready to proceed.\n"
     ]
    }
   ],
   "source": [
    "required_files = ['model.py', 'data_generator.py', 'losses.py']\n",
    "optional_files = ['main.py', 'train.py']\n",
    "\n",
    "print(\"Checking required files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_present = True\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úì {file} - Found\")\n",
    "    else:\n",
    "        print(f\"‚úó {file} - MISSING\")\n",
    "        all_present = False\n",
    "\n",
    "print(\"\\nChecking optional files...\")\n",
    "for file in optional_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úì {file} - Found\")\n",
    "    else:\n",
    "        print(f\"- {file} - Not present (optional)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "if all_present:\n",
    "    print(\"‚úì All required files present! Ready to proceed.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: Some required files are missing!\")\n",
    "    print(\"Please check your repository structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ozaeh-FTDpjW"
   },
   "source": [
    "## Step 6: Configure AWS Credentials\n",
    "\n",
    "**IMPORTANT SECURITY STEPS:**\n",
    "\n",
    "1. Click the **üîë Secrets** icon in the left sidebar\n",
    "2. Add these secrets:\n",
    "   - Name: `AWS_ACCESS_KEY_ID`, Value: Your AWS access key\n",
    "   - Name: `AWS_SECRET_ACCESS_KEY`, Value: Your AWS secret key\n",
    "3. Enable \"Notebook access\" for both secrets\n",
    "\n",
    "**Never hardcode credentials in notebooks!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lihTBrWjDpjX",
    "outputId": "148473eb-334a-4ee8-bfaa-5aadcb5300e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring AWS credentials...\n",
      "------------------------------------------------------------\n",
      "‚úì AWS credentials loaded from Colab Secrets\n",
      "‚úì Access Key ID: AKIAYAYR...\n"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "print(\"Configuring AWS credentials...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Get credentials from Colab Secrets\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "    print(\"‚úì AWS credentials loaded from Colab Secrets\")\n",
    "    print(\"‚úì Access Key ID: \" + os.environ['AWS_ACCESS_KEY_ID'][:8] + \"...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚úó Error loading AWS credentials from Colab Secrets\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nPlease add AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to Colab Secrets (üîë icon)\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGkgc6pYDpjX"
   },
   "source": [
    "## Step 7: Download Dataset from AWS S3\n",
    "\n",
    "**‚ö†Ô∏è UPDATE YOUR S3 BUCKET DETAILS BELOW**\n",
    "\n",
    "This will download your 7.95 GB dataset to Colab's local storage.  \n",
    "Estimated time: 10-15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmwc78FGDpjX",
    "outputId": "3d51af2e-bc37-4e3a-c242-d29bfbeb91aa"
   },
   "outputs": [],
   "source": "# ============================================================================\n# UPDATE THESE WITH YOUR S3 DETAILS\n# ============================================================================\nS3_BUCKET = 'your-brats2020-data'           # Your S3 bucket name\nS3_PATH = 'archive/BraTS2020_training_data/content/data'               # Path to data in S3 (no leading/trailing slashes)\nAWS_REGION = 'eu-central-1'                 # Your bucket's region\n# ============================================================================\n\nLOCAL_PATH = '/content/brainTumorData'\nPREPROCESSED_PATH = '/content/brainTumorData_preprocessed'\n\nprint(\"=\" * 60)\nprint(\"DOWNLOADING DATASET FROM AWS S3\")\nprint(\"=\" * 60)\n\n# Set AWS region\nos.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n\nprint(f\"\\nSource: s3://{S3_BUCKET}/{S3_PATH}\")\nprint(f\"Destination: {LOCAL_PATH}\")\nprint(f\"Region: {AWS_REGION}\")\nprint(f\"Dataset size: 7.95 GB\")\nprint(f\"Estimated time: 10-15 minutes\")\nprint(\"-\" * 60)\nprint(\"Starting download...\\n\")\n\n# Create local directory\n!mkdir -p {LOCAL_PATH}\n\n# Download using AWS CLI sync (shows progress)\n!aws s3 sync s3://{S3_BUCKET}/{S3_PATH} {LOCAL_PATH}\n\n# Verify download\nprint(\"\\n\" + \"=\" * 60)\nif os.path.exists(LOCAL_PATH):\n    # Count files\n    file_count = sum([len(files) for r, d, files in os.walk(LOCAL_PATH)])\n\n    # Calculate size\n    total_size = sum(\n        os.path.getsize(os.path.join(dirpath, filename))\n        for dirpath, dirnames, filenames in os.walk(LOCAL_PATH)\n        for filename in filenames\n    ) / (1024**3)  # Convert to GB\n\n    print(\"‚úì DOWNLOAD COMPLETE!\")\n    print(\"=\" * 60)\n    print(f\"Location: {LOCAL_PATH}\")\n    print(f\"Files downloaded: {file_count:,}\")\n    print(f\"Total size: {total_size:.2f} GB\")\n\n    # Show sample files\n    print(\"\\nSample files:\")\n    !ls {LOCAL_PATH} | head -10\n\n    # Check disk space after download\n    print(\"\\nüì¶ Disk Usage After Download:\")\n    !df -h /content | grep -E 'Filesystem|/content'\nelse:\n    print(\"‚úó DOWNLOAD FAILED!\")\n    print(\"Please check:\")\n    print(\"  1. S3 bucket name is correct\")\n    print(\"  2. S3 path is correct\")\n    print(\"  3. AWS credentials have read permissions\")\n    print(\"  4. AWS region is correct\")\n    raise FileNotFoundError(f\"Data not found at {LOCAL_PATH}\")\n\nprint(\"=\" * 60)\n\n# PREPROCESSING STEP\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PREPROCESSING DATA (Downsampling to 160x160x96)\")\nprint(\"=\" * 60)\nprint(\"This will reduce data size by ~50% for faster training\")\nprint(\"Estimated time: 15-20 minutes\")\nprint(\"-\" * 60)\n\n!python preprocess_data.py \\\n    --input_dir {LOCAL_PATH} \\\n    --output_dir {PREPROCESSED_PATH} \\\n    --num_volumes 369 \\\n    --num_slices 155 \\\n    --target_height 160 \\\n    --target_width 160 \\\n    --target_slices 96\n\n# Set data path to preprocessed directory\nDATA_PATH = PREPROCESSED_PATH\nprint(f\"\\n‚úì DATA_PATH set to: {DATA_PATH} (preprocessed)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvUvfQspDpjX"
   },
   "source": [
    "## Step 8: Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4B8Qtn5DpjX",
    "outputId": "1bd6d61e-9432-47c0-f43f-10906feb4926"
   },
   "outputs": [],
   "source": "import sys\n\n# Ensure repository is in Python path\nrepo_dir = f'/content/{repo_name}'\nif repo_dir not in sys.path:\n    sys.path.insert(0, repo_dir)\n\nprint(f\"Python path includes: {repo_dir}\")\nprint(f\"Working directory: {os.getcwd()}\")\nprint(\"-\" * 60)\n\n# Import your modules\ntry:\n    from model import MProtoNet3D_Segmentation_Keras\n    from data_generator import MRIDataGenerator\n    from losses import FocalLoss, CombinedLoss\n    from tensorflow import keras\n    import numpy as np\n\n    print(\"‚úì All modules imported successfully!\")\nexcept ImportError as e:\n    print(f\"‚úó Import error: {e}\")\n    print(\"\\nDebugging info:\")\n    print(\"Files in repository:\")\n    !ls -la\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC9abzWEDpjX"
   },
   "source": [
    "## Step 9: Training Configuration\n",
    "\n",
    "Adjust these parameters based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9dYh8gsDpjY",
    "outputId": "3d00992a-b5e1-45f5-e760-b07b47a2944a"
   },
   "outputs": [],
   "source": "# ============================================================================\n# TRAINING CONFIGURATION - ADJUST AS NEEDED\n# ============================================================================\n\n# Data configuration\nBATCH_SIZE = 4          # Increased batch size due to smaller volumes\nSPLIT_RATIO = 0.2       # 20% for validation\nRANDOM_STATE = 42\nNUM_VOLUMES = 369       # Total number of volumes\n\n# Volume dimensions (PREPROCESSED DATA)\nD = 96                  # Depth (number of slices) - reduced from 155\nH = 160                 # Height - reduced from 240\nW = 160                 # Width - reduced from 240\nC = 4                   # Channels (FLAIR, T1, T1ce, T2)\n\n# Model configuration\nNUM_CLASSES = 3         # GD enhancing tumor, peritumoral edema, non-enhancing tumor core\nPROTOTYPE_SHAPE = (21, 128, 1, 1, 1)  # 21/3 = 7 prototypes per class\n\n# Training settings\nEPOCHS = 100            # Number of training epochs (early stopping will handle when to stop)\nLEARNING_RATE = 0.0001  # Initial learning rate\n\n# ============================================================================\n\nINPUT_SHAPE = (D, H, W, C)\n\nprint(\"=\" * 60)\nprint(\"TRAINING CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"Data path: {DATA_PATH}\")\nprint(f\"Input shape: {INPUT_SHAPE}\")\nprint(f\"Number of classes: {NUM_CLASSES}\")\nprint(f\"Prototypes per class: {PROTOTYPE_SHAPE[0] // NUM_CLASSES}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Epochs: {EPOCHS}\")\nprint(f\"Learning rate: {LEARNING_RATE}\")\nprint(f\"Total volumes: {NUM_VOLUMES}\")\nprint(f\"Train/Val split: {int((1-SPLIT_RATIO)*100)}% / {int(SPLIT_RATIO*100)}%\")\nprint(f\"\\nüìä Data Reduction Benefits:\")\nprint(f\"  - Original size: 155 √ó 240 √ó 240 = 8,928,000 voxels\")\nprint(f\"  - Preprocessed size: {D} √ó {H} √ó {W} = {D*H*W:,} voxels\")\nprint(f\"  - Reduction: ~51% smaller\")\nprint(f\"  - Batch size increased from 2 to {BATCH_SIZE}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2er01OhDDpjY"
   },
   "source": [
    "## Step 10: Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRl36gVqDpjY",
    "outputId": "386f42f6-d654-4536-a203-b4b7cc275793"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating data generators...\n",
      "------------------------------------------------------------\n",
      "MRIDataGenerator: Initializing for H5 files from: /content/brainTumorData\n",
      "MRIDataGenerator: Found 369 unique volume IDs (0 to 368).\n",
      "MRIDataGenerator: train subset has 295 volumes (each containing 155 slices).\n",
      "MRIDataGenerator: Initializing for H5 files from: /content/brainTumorData\n",
      "MRIDataGenerator: Found 369 unique volume IDs (0 to 368).\n",
      "MRIDataGenerator: val subset has 74 volumes (each containing 155 slices).\n",
      "\n",
      "‚úì Training batches: 147\n",
      "‚úì Validation batches: 37\n",
      "\n",
      "Estimated time per epoch (T4 GPU): ~368 minutes\n",
      "Total estimated training time: ~61.2 hours\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating data generators...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "train_generator = MRIDataGenerator(\n",
    "    DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_slices=D,\n",
    "    num_volumes=NUM_VOLUMES,\n",
    "    split_ratio=SPLIT_RATIO,\n",
    "    subset='train',\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "validation_generator = MRIDataGenerator(\n",
    "    DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_slices=D,\n",
    "    num_volumes=NUM_VOLUMES,\n",
    "    split_ratio=SPLIT_RATIO,\n",
    "    subset='val',\n",
    "    shuffle=False,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Training batches: {len(train_generator)}\")\n",
    "print(f\"‚úì Validation batches: {len(validation_generator)}\")\n",
    "print(f\"\\nEstimated time per epoch (T4 GPU): ~{len(train_generator) * 2.5:.0f} minutes\")\n",
    "print(f\"Total estimated training time: ~{len(train_generator) * 2.5 * EPOCHS / 60:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMSwN2C4DpjY"
   },
   "source": [
    "## Step 11: Build and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYeahRUfDpjY",
    "outputId": "7212e067-f1c7-4e95-88ef-285cfa9a0f2a"
   },
   "outputs": [],
   "source": "print(\"Building model...\")\nprint(\"-\" * 60)\n\n# Build the MProtoNet3D model\nmodel = MProtoNet3D_Segmentation_Keras(\n    in_size=INPUT_SHAPE,\n    num_classes=NUM_CLASSES,\n    prototype_shape=PROTOTYPE_SHAPE,\n    features='resnet50_ri',\n    f_dist='l2'\n)\n\nprint(\"‚úì Model architecture created!\")\n\n# Build the model explicitly to enable parameter counting\nprint(\"Initializing model layers...\")\nmodel.build(input_shape=(None,) + INPUT_SHAPE)\nprint(\"‚úì Model built successfully!\")\n\n# Setup optimizer and loss\noptimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n\n# Use Combined Focal + Dice Loss (RECOMMENDED for medical segmentation)\nloss_fn = CombinedLoss(focal_weight=0.5, dice_weight=0.5, gamma=1.0, alpha=0.25)\n\n# Compile model with comprehensive metrics\nmodel.compile(\n    optimizer=optimizer,\n    loss=loss_fn,\n    metrics=[\n        'accuracy',\n        keras.metrics.MeanIoU(num_classes=NUM_CLASSES, name='mean_iou'),\n        keras.metrics.Precision(name='precision'),\n        keras.metrics.Recall(name='recall')\n    ]\n)\n\nprint(\"‚úì Model compiled successfully!\")\nprint(\"\\nLoss function: Combined Focal + Dice Loss\")\nprint(\"  - Focal loss gamma: 1.0\")\nprint(\"  - Focal loss alpha: 0.25\")\nprint(\"  - Loss weights: 50% Focal + 50% Dice\")\nprint(\"\\nMetrics tracked:\")\nprint(\"  - Accuracy (overall voxel accuracy)\")\nprint(\"  - Mean IoU (Intersection over Union per class)\")\nprint(\"  - Precision & Recall\")\n\n# Count parameters\ntry:\n    total_params = model.count_params()\n    print(f\"\\nTotal trainable parameters: {total_params:,}\")\nexcept:\n    print(\"\\nNote: Parameter count will be available after first training step\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaWZxIwoDpjY"
   },
   "source": [
    "## Step 12: Setup Callbacks and Checkpointing\n",
    "\n",
    "**IMPORTANT:** We'll save checkpoints to Google Drive for persistence across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZIOq_U0DpjY",
    "outputId": "28534541-1b9e-4849-d2f6-7fd0e6c038bd"
   },
   "outputs": [],
   "source": "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\nimport datetime\n\n# Mount Google Drive for checkpoint storage\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Create directories\ncheckpoint_dir = '/content/checkpoints'\nlogs_dir = '/content/logs'\n!mkdir -p {checkpoint_dir}\n!mkdir -p {logs_dir}\n\n# Google Drive checkpoint directory (for persistence)\ndrive_checkpoint_dir = '/content/drive/MyDrive/brain_tumor_checkpoints'\n!mkdir -p {drive_checkpoint_dir}\n\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ncallbacks = [\n    # Save best model locally\n    ModelCheckpoint(\n        filepath=f'{checkpoint_dir}/best_model.keras',\n        monitor='val_loss',\n        save_best_only=True,\n        mode='min',\n        verbose=1\n    ),\n\n    # Save best model to Google Drive (IMPORTANT for persistence)\n    ModelCheckpoint(\n        filepath=f'{drive_checkpoint_dir}/best_model_{timestamp}.keras',\n        monitor='val_loss',\n        save_best_only=True,\n        mode='min',\n        verbose=1\n    ),\n\n    # Save periodic checkpoints to Google Drive (every epoch)\n    ModelCheckpoint(\n        filepath=f'{drive_checkpoint_dir}/checkpoint_epoch_{{epoch:02d}}_{timestamp}.keras',\n        save_freq='epoch',\n        save_best_only=False,\n        verbose=1\n    ),\n\n    # Early stopping - wait 10 epochs before stopping\n    EarlyStopping(\n        monitor='val_loss',\n        patience=10,\n        restore_best_weights=True,\n        verbose=1\n    ),\n\n    # Reduce learning rate on plateau - wait 5 epochs before reducing\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=5,\n        min_lr=1e-7,\n        verbose=1\n    ),\n\n    # TensorBoard logging\n    TensorBoard(\n        log_dir=f'{logs_dir}/{timestamp}',\n        histogram_freq=1,\n        write_graph=True\n    ),\n\n    # CSV Logger\n    CSVLogger(\n        filename=f'{drive_checkpoint_dir}/training_log_{timestamp}.csv',\n        append=True\n    )\n]\n\nprint(\"‚úì Callbacks configured!\")\nprint(f\"  - Local checkpoints: {checkpoint_dir}\")\nprint(f\"  - Drive backups: {drive_checkpoint_dir}\")\nprint(f\"  - TensorBoard logs: {logs_dir}\")\nprint(f\"  - Timestamp: {timestamp}\")\nprint(\"\\nCallback settings:\")\nprint(\"  - EarlyStopping: patience=10 epochs\")\nprint(\"  - ReduceLROnPlateau: patience=5 epochs, factor=0.5\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tIp3H38DpjZ"
   },
   "source": [
    "## Step 13: Train the Model\n",
    "\n",
    "**This will take several hours. Keep the browser tab active to prevent disconnection!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Xf_bX7XrDpjZ",
    "outputId": "372b5695-84b2-448c-e92f-499f6e564f04"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Training for 10 epochs\n",
      "Estimated total time: ~61.2 hours (T4 GPU)\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT: Keep this browser tab active to prevent disconnection!\n",
      "‚ö†Ô∏è Models are being saved to Google Drive automatically\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 4)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 3)\n",
      "After padding - Image shape: (160, 240, 240, 4), Mask shape: (160, 240, 240, 3)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 4)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 3)\n",
      "After padding - Image shape: (160, 240, 240, 4), Mask shape: (160, 240, 240, 3)\n",
      "Final batch shapes - Images: (2, 160, 240, 240, 4), Masks: (2, 160, 240, 240, 3)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 4)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 3)\n",
      "After padding - Image shape: (160, 240, 240, 4), Mask shape: (160, 240, 240, 3)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 4)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 3)\n",
      "After padding - Image shape: (160, 240, 240, 4), Mask shape: (160, 240, 240, 3)\n",
      "Final batch shapes - Images: (2, 160, 240, 240, 4), Masks: (2, 160, 240, 240, 3)\n",
      "Epoch 1/10\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 4)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 3)\n",
      "After padding - Image shape: (160, 240, 240, 4), Mask shape: (160, 240, 240, 3)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 4)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 3)\n",
      "After padding - Image shape: (160, 240, 240, 4), Mask shape: (160, 240, 240, 3)\n",
      "Final batch shapes - Images: (2, 160, 240, 240, 4), Masks: (2, 160, 240, 240, 3)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 4)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 3)\n",
      "After padding - Image shape: (160, 240, 240, 4), Mask shape: (160, 240, 240, 3)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 4)\n",
      "Padding volume: 155 -> 160 (pad_before=2, pad_after=3)\n",
      "Padded volume shape: (160, 240, 240, 3)\n",
      "After padding - Image shape: (160, 240, 240, 4), Mask shape: (160, 240, 240, 3)\n",
      "Final batch shapes - Images: (2, 160, 240, 240, 4), Masks: (2, 160, 240, 240, 3)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipython-input-2317829315.py\", line 11, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nFailed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bw-input.16 = (f32[2,64,160,240,240]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[2,16,160,240,240]{4,3,2,1,0} %bitcast.16789, f32[16,64,3,3,3]{4,3,2,1,0} %bitcast.16726), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv3DBackpropInputV2\" op_name=\"gradient_tape/m_proto_net3d__segmentation__keras_1/final_processing_1/convolution/Conv3DBackpropInputV2\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 4735369216 bytes. [tf-allocator-allocation-error='']\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_12651]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2317829315.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipython-input-2317829315.py\", line 11, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nFailed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bw-input.16 = (f32[2,64,160,240,240]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[2,16,160,240,240]{4,3,2,1,0} %bitcast.16789, f32[16,64,3,3,3]{4,3,2,1,0} %bitcast.16726), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv3DBackpropInputV2\" op_name=\"gradient_tape/m_proto_net3d__segmentation__keras_1/final_processing_1/convolution/Conv3DBackpropInputV2\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 4735369216 bytes. [tf-allocator-allocation-error='']\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_12651]"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training for {EPOCHS} epochs\")\n",
    "print(f\"Estimated total time: ~{len(train_generator) * 2.5 * EPOCHS / 60:.1f} hours (T4 GPU)\")\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Keep this browser tab active to prevent disconnection!\")\n",
    "print(\"‚ö†Ô∏è Models are being saved to Google Drive automatically\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì TRAINING COMPLETED!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUH7sYkCDpjZ"
   },
   "source": [
    "## Step 14: Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byhSqEvrDpjZ"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfig.suptitle('Training Metrics Over Time', fontsize=16, fontweight='bold')\n\n# Loss plot\naxes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)\naxes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\naxes[0, 0].set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\naxes[0, 0].set_xlabel('Epoch', fontsize=12)\naxes[0, 0].set_ylabel('Loss', fontsize=12)\naxes[0, 0].legend(fontsize=10)\naxes[0, 0].grid(True, alpha=0.3)\n\n# Accuracy plot\naxes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\naxes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\naxes[0, 1].set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\naxes[0, 1].set_xlabel('Epoch', fontsize=12)\naxes[0, 1].set_ylabel('Accuracy', fontsize=12)\naxes[0, 1].legend(fontsize=10)\naxes[0, 1].grid(True, alpha=0.3)\n\n# Mean IoU plot\naxes[0, 2].plot(history.history['mean_iou'], label='Training Mean IoU', linewidth=2)\naxes[0, 2].plot(history.history['val_mean_iou'], label='Validation Mean IoU', linewidth=2)\naxes[0, 2].set_title('Mean IoU Over Time', fontsize=14, fontweight='bold')\naxes[0, 2].set_xlabel('Epoch', fontsize=12)\naxes[0, 2].set_ylabel('Mean IoU', fontsize=12)\naxes[0, 2].legend(fontsize=10)\naxes[0, 2].grid(True, alpha=0.3)\n\n# Precision plot\naxes[1, 0].plot(history.history['precision'], label='Training Precision', linewidth=2)\naxes[1, 0].plot(history.history['val_precision'], label='Validation Precision', linewidth=2)\naxes[1, 0].set_title('Precision Over Time', fontsize=14, fontweight='bold')\naxes[1, 0].set_xlabel('Epoch', fontsize=12)\naxes[1, 0].set_ylabel('Precision', fontsize=12)\naxes[1, 0].legend(fontsize=10)\naxes[1, 0].grid(True, alpha=0.3)\n\n# Recall plot\naxes[1, 1].plot(history.history['recall'], label='Training Recall', linewidth=2)\naxes[1, 1].plot(history.history['val_recall'], label='Validation Recall', linewidth=2)\naxes[1, 1].set_title('Recall Over Time', fontsize=14, fontweight='bold')\naxes[1, 1].set_xlabel('Epoch', fontsize=12)\naxes[1, 1].set_ylabel('Recall', fontsize=12)\naxes[1, 1].legend(fontsize=10)\naxes[1, 1].grid(True, alpha=0.3)\n\n# Hide the last subplot (we have 5 metrics, 6 subplots)\naxes[1, 2].axis('off')\n\nplt.tight_layout()\nplt.savefig(f'{drive_checkpoint_dir}/training_history_{timestamp}.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"‚úì Training history saved to Google Drive\")\n\n# Print final metrics\nprint(\"\\n\" + \"=\" * 60)\nprint(\"FINAL TRAINING METRICS\")\nprint(\"=\" * 60)\nprint(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\nprint(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\nprint(f\"Final training mean IoU: {history.history['mean_iou'][-1]:.4f}\")\nprint(f\"Final training precision: {history.history['precision'][-1]:.4f}\")\nprint(f\"Final training recall: {history.history['recall'][-1]:.4f}\")\nprint(\"\\nFINAL VALIDATION METRICS\")\nprint(\"=\" * 60)\nprint(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\nprint(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\nprint(f\"Final validation mean IoU: {history.history['val_mean_iou'][-1]:.4f}\")\nprint(f\"Final validation precision: {history.history['val_precision'][-1]:.4f}\")\nprint(f\"Final validation recall: {history.history['val_recall'][-1]:.4f}\")\nprint(\"\\nBEST VALIDATION METRICS\")\nprint(\"=\" * 60)\nprint(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\nprint(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\nprint(f\"Best validation mean IoU: {max(history.history['val_mean_iou']):.4f}\")\nprint(f\"Best validation precision: {max(history.history['val_precision']):.4f}\")\nprint(f\"Best validation recall: {max(history.history['val_recall']):.4f}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7_DBWPLDpjZ"
   },
   "source": [
    "## Step 15: Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xy51i0sYDpjZ"
   },
   "outputs": [],
   "source": "# Save final model to Google Drive\nfinal_model_path = f'{drive_checkpoint_dir}/final_model_{timestamp}.keras'\nmodel.save(final_model_path)\nprint(f\"‚úì Final model saved to: {final_model_path}\")\n\n# Also save locally\nmodel.save('/content/final_model.keras')\nprint(f\"‚úì Final model also saved locally to: /content/final_model.keras\")\n\nprint(\"\\n‚úì All models safely stored in Google Drive!\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"To load the best model:\")\nprint(\"=\" * 60)\nprint(\"from losses import CombinedLoss\")\nprint(\"model = keras.models.load_model('best_model.keras',\")\nprint(\"                                 custom_objects={'CombinedLoss': CombinedLoss})\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_-cdSGTDpjZ"
   },
   "source": [
    "## Step 16: Test Prediction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTADMqc4Dpja"
   },
   "outputs": [],
   "source": [
    "# Get a sample from validation set\n",
    "print(\"Loading sample for prediction...\")\n",
    "sample_x, sample_y = validation_generator[0]\n",
    "\n",
    "print(f\"Input shape: {sample_x.shape}\")\n",
    "print(f\"Label shape: {sample_y.shape}\")\n",
    "\n",
    "# Make prediction\n",
    "print(\"\\nGenerating prediction...\")\n",
    "prediction = model.predict(sample_x, verbose=0)\n",
    "print(f\"Prediction shape: {prediction.shape}\")\n",
    "\n",
    "# Visualize middle slice\n",
    "slice_idx = D // 2  # Middle slice\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle(f'Brain Tumor Segmentation - Slice {slice_idx}', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Input modalities\n",
    "axes[0, 0].imshow(sample_x[0, slice_idx, :, :, 0], cmap='gray')\n",
    "axes[0, 0].set_title('FLAIR', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(sample_x[0, slice_idx, :, :, 1], cmap='gray')\n",
    "axes[0, 1].set_title('T1', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(sample_x[0, slice_idx, :, :, 2], cmap='gray')\n",
    "axes[0, 2].set_title('T1ce', fontsize=12)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Ground truth and prediction\n",
    "axes[1, 0].imshow(sample_x[0, slice_idx, :, :, 3], cmap='gray')\n",
    "axes[1, 0].set_title('T2', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(np.argmax(sample_y[0, slice_idx], axis=-1), cmap='jet', vmin=0, vmax=NUM_CLASSES-1)\n",
    "axes[1, 1].set_title('Ground Truth', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(np.argmax(prediction[0, slice_idx], axis=-1), cmap='jet', vmin=0, vmax=NUM_CLASSES-1)\n",
    "axes[1, 2].set_title('Prediction', fontsize=12)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{drive_checkpoint_dir}/prediction_visualization_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Prediction visualization saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dja2jVLTDpja"
   },
   "source": [
    "## Step 17: Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUQO62VzDpja"
   },
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\" \" * 20 + \"TRAINING COMPLETE!\" + \" \" * 20)\nprint(\"=\" * 70)\nprint(\"\\n‚úì Your trained models are safely stored in Google Drive:\")\nprint(f\"  üìÅ {drive_checkpoint_dir}/\")\nprint(\"\\n‚úì Files saved:\")\nprint(f\"  - best_model_{timestamp}.keras\")\nprint(f\"  - final_model_{timestamp}.keras\")\nprint(f\"  - checkpoint_epoch_*.keras (periodic checkpoints)\")\nprint(f\"  - training_log_{timestamp}.csv\")\nprint(f\"  - training_history_{timestamp}.png\")\nprint(f\"  - prediction_visualization_{timestamp}.png\")\nprint(\"\\nüìä Training Configuration Summary:\")\nprint(f\"  - Epochs: {EPOCHS} (early stopping enabled)\")\nprint(f\"  - Batch size: {BATCH_SIZE}\")\nprint(f\"  - Learning rate: {LEARNING_RATE}\")\nprint(f\"  - Loss: Combined Focal + Dice Loss (50%/50%)\")\nprint(f\"  - Metrics: Accuracy, Mean IoU, Precision, Recall\")\nprint(f\"  - Early stopping patience: 10 epochs\")\nprint(f\"  - LR reduction patience: 5 epochs\")\nprint(\"\\nüìä View TensorBoard logs:\")\nprint(f\"  Run: %load_ext tensorboard\")\nprint(f\"       %tensorboard --logdir {logs_dir}\")\nprint(\"\\nüéØ Next Steps:\")\nprint(\"  1. Evaluate model on test set\")\nprint(\"  2. Perform hyperparameter tuning if needed\")\nprint(\"  3. Generate more visualizations\")\nprint(\"  4. Export model for deployment\")\nprint(\"\\nüíæ All important files are backed up to Google Drive!\")\nprint(\"=\" * 70)\n\n# List all saved files\nprint(\"\\nFiles in Google Drive checkpoint directory:\")\n!ls -lh {drive_checkpoint_dir}"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3KNcSjSDpja"
   },
   "source": [
    "## Optional: Launch TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qWd0N10Dpja"
   },
   "outputs": [],
   "source": [
    "# Uncomment to launch TensorBoard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir {logs_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT1RScx7Dpjb"
   },
   "source": [
    "## Optional: Download Files Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvrvGyNpDpjb"
   },
   "outputs": [],
   "source": [
    "# Uncomment to download files to your computer\n",
    "# from google.colab import files\n",
    "# files.download('/content/final_model.keras')\n",
    "# files.download(f'{drive_checkpoint_dir}/training_history_{timestamp}.png')\n",
    "# files.download(f'{drive_checkpoint_dir}/prediction_visualization_{timestamp}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6Z13p8WDpjb"
   },
   "source": "## Notes and Tips\n\n### Dataset Storage\n- Your 7.95 GB dataset is stored on **disk** (local SSD)\n- Only **one batch** (~400 MB with batch_size=2) is loaded in **RAM** at a time\n- This is very efficient and won't cause memory issues\n\n### Training Configuration\n- **Epochs**: 100 (early stopping will stop training if no improvement for 10 epochs)\n- **Batch Size**: 2 (provides better batch normalization stability)\n- **Loss Function**: Combined Focal + Dice Loss (optimal for medical segmentation)\n  - Focal Loss: Handles class imbalance\n  - Dice Loss: Optimizes overlap between predicted and actual segmentation\n- **Metrics**: Accuracy, Mean IoU, Precision, Recall for comprehensive evaluation\n\n### Session Management\n- **Colab Free**: 12-hour session limit\n- **Colab Pro**: 24-hour session limit\n- Keep browser tab active to prevent disconnection\n- All checkpoints are saved to Google Drive automatically every epoch\n\n### Resuming Training\nIf your session disconnects, you can resume:\n```python\nfrom losses import CombinedLoss\n\n# Load the last checkpoint\ncheckpoint_path = f'{drive_checkpoint_dir}/checkpoint_epoch_05_{timestamp}.keras'\nmodel = keras.models.load_model(checkpoint_path, custom_objects={'CombinedLoss': CombinedLoss})\n\n# Continue training\nhistory = model.fit(\n    train_generator,\n    epochs=100,\n    initial_epoch=5,  # Start from where you left off\n    validation_data=validation_generator,\n    callbacks=callbacks\n)\n```\n\n### Loading Best Model for Inference\n```python\nfrom losses import CombinedLoss\n\n# Load best model\nbest_model_path = f'{drive_checkpoint_dir}/best_model_{timestamp}.keras'\nmodel = keras.models.load_model(best_model_path, custom_objects={'CombinedLoss': CombinedLoss})\n\n# Make predictions\npredictions = model.predict(validation_generator)\n```\n\n### Performance Tips\n- Use **Colab Pro** for better GPUs (V100/A100) and longer sessions\n- Monitor GPU usage with: `!nvidia-smi`\n- Check disk usage with: `!df -h /content`\n- Monitor RAM with: `!free -h`\n- All metrics are logged to CSV for offline analysis"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}